---
title: "PML Assignment"
author: "Paul Robberson"
date: "Thursday, October 22, 2015"
output: html_document
---
Summary

We are concerned with being able to predict whether the simple exercise of curling a light dumbbell is being done correctly or incorrectly.  A write-up of this experiment can be found at 
http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf.

Six participants were asked to perform 10 repetitions of a dumbbell curl first as specified by a weight-lifting trainer (although the exact specifications are not mentioned), then 10 each of 4 commonly mistaken ways of doing the exercise.  These are captured in the data in the variable "classe" according to the following descriptions: 
.  exactly according to the speci???cation (Class A)
.	throwing the elbows to the front (Class B)
.	lifting the dumbbell only halfway (Class C)
.	lowering the dumbbell only halfway (Class D)
.	throwing the hips to the front (Class E).

Four Razor 9-degrees of freedom sensors were mounted on dumbbell, forearm, arm, and belt.  Documentation on these sensors can be found at: https://www.sparkfun.com/products/10736.  These sensors each contain 3 devices, each of which took their measurements in three dimensions (x, y, z).
.	accelerometer, measuring acceleration including gravity - "accel" in the data
.	gyroscope, measuring angular motion - "gyros" in the data
.	magnetometer, measuring distance - "magnet" in the data

Data points were captured at 45hz in sliding time chunks ranging from .5 seconds to 2.5 seconds and saved in a file containing 19,622 observations of 160 variables.

The following is a description of building a predictive model that identifies the "classe" using a subset of the other variables.

Description

Exploring the data by reading the presented data file, "pml-training.csv", and using the View() command shows that many of the columns have nearly all "NA" or are blank.  

>trn <- read.csv("pml-training.csv")

Examining the structure of trn, we find:

> str(trn)
'data.frame':	19622 obs. of  160 variables

The use of the View() command gives a good spreadsheet-like table of the data, but the number of columns and rows are limited.  Therefore, multiple View()'s are needed.

>View(trn[,1:30])
>View(trn[,31:60])
Etc.

It should be noted that the data set is small enough to be read by Excel and perused in its entirety.

It seems reasonable to eliminate the columns that are mostly empty or NA.  Further, there is no data dictionary, so a reasonable assumption is that the 36 basic readings (3 x,y,z readings, 3 sensors, 4 devices) are the primary ones to investigate, plus the outcome "classe".

>trndat <- select(trn, classe, starts_with("gyro"), starts_with("accel"), starts_with("magnet"))

The resulting trndat data frame has 19,622 observations of 37 variables.

The "pml-testing.csv" supplied file has 20 observations without the outcome "classe".  This is used to
test the model in the Prediction Assignment.  Therefore, to create cross validation the trndat data frame is partitioned into .75/.25 training/testing sets (dattrn and dattst).  

> trnindex <- createDataPartition(y=trndat$classe, p=0.75, list=FALSE)
> dattrn <- trndat[trnindex,]
> str(dattrn)

'data.frame':	14718 obs. of  37 variables

> dattst <- trndat[-trnindex,]
> str(dattst)

'data.frame':	4904 obs. of  37 variables:

A Random Forest training of "classe" against the other 36 variables is created. 

modfit <- train(dattrn$classe ~., method="rf", data=dattrn)

> modfit

Random Forest 

14718 samples
   36 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 14718, 14718, 14718, 14718, 14718, 14718, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
  
   2    0.9812954  0.9763349  0.002590328  0.003278947
  
  19    0.9774804  0.9715090  0.002358977  0.002984152
  
  36    0.9697265  0.9616962  0.003533743  0.004485859

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 

Based on the Accuracy (.9812954) reported in the modfit model, we would expect an out-sample-error rate of 2% or a bit more.

To gain a cross validation, the modfit model was then used to predict the outcomes from the testing set, dattst. The table below shows these predictions vs actual outcomes.  The predictions on the testing set are .98878 accurate (4844/4904), just slightly better than the optimal model chosen by Random Forest.

> pred <- predict(modfit, dattst)
> table(pred, dattst$classe)
    
pred    A    B    C    D    E
   A 1393   10    0    1    0
   B    0  935   11    0    0
   C    0    4  844   24    0
   D    1    0    0  776    5
   E    1    0    0    3  896


Modfit was then used to predict the outcomes for the 20 observations in the supplied pml-testing.csv file.

>tst <- read.csv("pml-testing.csv")

>pred <- predict(modfit, tst)

The predictions were submitted and verified to be 100% correct.

